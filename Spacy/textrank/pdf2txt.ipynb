{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "915563ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfinterp import PDFResourceManager\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.converter import PDFPageAggregator\n",
    "from collections import defaultdict\n",
    "from math import *\n",
    "import pdfminer\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d1ef1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createPDFDoc(fp):\n",
    "    parser = PDFParser(fp)\n",
    "    document = PDFDocument(parser, password='')\n",
    "    # Check if the document allows text extraction. If not, abort.\n",
    "    if not document.is_extractable:\n",
    "        raise ValueError(\"Not extractable\")\n",
    "    else:\n",
    "        return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3ba085a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDeviceInterpreter():\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    laparams = LAParams()\n",
    "    device = PDFPageAggregator(rsrcmgr, laparams=laparams)\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    return device, interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf1666c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = open('1908.07836.pdf', 'rb')\n",
    "document = createPDFDoc(fp)  # It will close the file, so no need of fp.close()\n",
    "device, interpreter = createDeviceInterpreter()\n",
    "pages = PDFPage.create_pages(document)\n",
    "interpreter.process_page(next(pages))\n",
    "layout = device.get_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "63d61c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_obj(objs):\n",
    "    js = defaultdict(lambda: defaultdict(lambda: {}))\n",
    "    for obj in objs:\n",
    "        # if it's a container, recurse\n",
    "        if isinstance(obj, pdfminer.layout.LTFigure):\n",
    "            parse_obj(obj._objs)\n",
    "        elif isinstance(obj, pdfminer.layout.LTTextBox):\n",
    "            for o in obj._objs:\n",
    "                if isinstance(o, pdfminer.layout.LTTextLine):\n",
    "                    text = o.get_text().strip()\n",
    "#                     if text.replace(\"\\n\", \"\").replace(\" \", \"\") == \"\": continue\n",
    "                    x0, y0 = int(o.x0), int(o.y0)\n",
    "                    if text not in js[y0][x0]:\n",
    "                        js[y0][x0][text] = {}\n",
    "                    js[y0][x0] = {\n",
    "                        \"bbox\": o.bbox,\n",
    "                        \"height\": floor(o.height),\n",
    "                        \"width\": o.width,\n",
    "                        \"text\": o.get_text()\n",
    "                    }\n",
    "\n",
    "    return js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "42c17162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 28], 255.9717979000003, True)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "js = parse_obj(layout._objs)\n",
    "font_size = sorted(set([js[y0][x0][\"height\"] for y0 in js for x0 in js[y0]]))\n",
    "max_widths = max([js[y0][x0][\"width\"] for y0 in js for x0 in js[y0] \n",
    "                  if js[y0][x0][\"height\"] not in [font_size[-1], font_size[-2]] ])\n",
    "side_by_side = True if max_widths < layout.width * 0.65 else False\n",
    "font_size, max_widths, side_by_side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "014faf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_sections, r_sections = [(\"\", (0, np.inf))], [(\"\", (0, np.inf))]\n",
    "for y0 in reversed(sorted(js)):\n",
    "    for x0 in sorted(js[y0]):\n",
    "        text = js[y0][x0][\"text\"].strip()\n",
    "        if len(text.strip()) <= 1: continue\n",
    "        if x0 < max_widths:\n",
    "            if l_sections[-1][1][1] - y0 > 18: text = '\\n\\n' + text.strip()\n",
    "            l_sections.append((text, (x0, y0)))\n",
    "        else: \n",
    "            if r_sections[-1][1][1] - y0 > 18: text = '\\n\\n' + text.strip()\n",
    "            r_sections.append((text, (x0, y0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b6c7a855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "Jianbin Tang Antonio Jimeno Yepes IBM Research Australia 60 City Road, Southbank VIC 3006, Australia jbtang@au1.ibm.com antonio.jimeno@au1.ibm.com \n",
      "\n",
      "a slow and expensive process, which is a stepping curve when willing to use these techniques in new domains. In this work, we propose a method to automatically annotate the document layout of over 1 million PubMed Central™ PDF articles and generate a high-quality document layout dataset called PubLayNet. The dataset contains over 360k page samples and covers typical document layout elements such as text, title, list, ﬁgure, and table. Then, we evaluate deep object detection neural networks on the PubLayNet dataset and the performance of ﬁne tuning the networks on existing small manually annotated corpora. We show that the automatically annotated dataset is suitable to train a model to recognize the layout of scientiﬁc articles, and the model pre-trained on the dataset can be a more effective base in transfer learning. \n",
      "\n",
      "II. RELATED WORK Existing datasets for document layout analysis rely on manual annotation. Some of these datasets are used in document processing challenges. Examples of these efforts are available in several ICDAR challenges [7], which cover as well complex layouts [8], [9]. The US NIH National Library of Medicine has provided the Medical Article Records Groundtruth (MARG)1, which are obtained from scanned article pages. In addition to document layout, further understanding of the document content has been studied in the evaluation of table detection methods, e.g. [10], [11]. Examples include table detection from document images using heuristics [12], vertical arrangement of text blocks [13] and deep learning methods [14]–[17]. Overall, the datasets are of limited size, just several hundred pages, which is mostly due to the need for manual annotation. In the next section, we describe how multiple versions of the same document from PubMed Central™ are used to automatically generate document layout annotations for over 1 million documents. III. AUTOMATIC ANNOTATION OF DOCUMENT LAYOUT To overcome the lack of training data, we used a large document collection from PubMed Central™ Open Access \n",
      "\n",
      "1https://ceb.nlm.nih.gov/inactive-communications-engineering-branch-projects/ medical-article-records-groundtruth-marg/\n"
     ]
    }
   ],
   "source": [
    "# print(\" \".join([text for text, _ in l_sections]))\n",
    "print(\" \".join([text for text, _ in r_sections]))\n",
    "# r_sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8132ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
