{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa413b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_lg\n",
    "# from spacy.lang.en import English\n",
    "# nlp = English()\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ab02e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Let's learn Spacy for $0 billion\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1, $0 billion), {})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tagger (part of speech tagger) -> Token.tag, Token,pos\n",
    "# Parser (Dependency parser) -> Token.dep, Token,head, Doc.sents, Doc.noun_chunks\n",
    "# ner (Named entity recognizer) -> Doc.ents, Token.ent_iob, Token.ent_type\n",
    "# textcat (Text classifier) -> Doc.cats\n",
    "\n",
    "doc = nlp(\"1. Let's learn Spacy for $0 billion\")\n",
    "# doc = nlp(\"An awesome tutorial\")\n",
    "print(doc.text)\n",
    "list(doc.sents), list(doc.noun_chunks)\n",
    "doc.ents, doc.cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "810373bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text(0): 1, Attributr: X, TAG: LS\n",
      "is_alpha: False, is_punctuation: False, like_number: True, is_currency:False\n",
      "ROOT\n",
      "\n",
      "Text(1): ., Attributr: PUNCT, TAG: .\n",
      "is_alpha: False, is_punctuation: True, like_number: False, is_currency:False\n",
      "punct\n",
      "\n",
      "Text(2): Let, Attributr: VERB, TAG: VB\n",
      "is_alpha: True, is_punctuation: False, like_number: False, is_currency:False\n",
      "ROOT\n",
      "\n",
      "Text(3): 's, Attributr: PRON, TAG: PRP\n",
      "is_alpha: False, is_punctuation: False, like_number: False, is_currency:False\n",
      "nsubj\n",
      "\n",
      "Text(4): learn, Attributr: VERB, TAG: VB\n",
      "is_alpha: True, is_punctuation: False, like_number: False, is_currency:False\n",
      "ccomp\n",
      "\n",
      "Text(5): Spacy, Attributr: PROPN, TAG: NNP\n",
      "is_alpha: True, is_punctuation: False, like_number: False, is_currency:False\n",
      "dobj\n",
      "\n",
      "Text(6): for, Attributr: ADP, TAG: IN\n",
      "is_alpha: True, is_punctuation: False, like_number: False, is_currency:False\n",
      "prep\n",
      "\n",
      "Text(7): $, Attributr: SYM, TAG: $\n",
      "is_alpha: False, is_punctuation: False, like_number: False, is_currency:True\n",
      "quantmod\n",
      "\n",
      "Text(8): 0, Attributr: NUM, TAG: CD\n",
      "is_alpha: False, is_punctuation: False, like_number: True, is_currency:False\n",
      "compound\n",
      "\n",
      "Text(9): billion, Attributr: NUM, TAG: CD\n",
      "is_alpha: True, is_punctuation: False, like_number: True, is_currency:False\n",
      "pobj\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# token.pos_: returns string, token.pos: returns: ID value\n",
    "for token in doc:\n",
    "    print(f\"Text({token.i}): {token.text}, Attributr: {token.pos_}, TAG: {token.tag_}\")\n",
    "    print(f\"is_alpha: {token.is_alpha}, is_punctuation: {token.is_punct}, like_number: {token.like_num}, is_currency:{token.is_currency}\")\n",
    "    print(f\"{token.dep_}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7adfaba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"\"\"\n",
    "A match is a tool for starting a fire. \n",
    "1. Typically, modern matches are made of small wooden sticks or stiff paper. \n",
    "2. It don't cost $1 million !\n",
    "3. One end is coated with a material that can be ignited by frictional heat generated by striking the match \n",
    "against a suitable surface. \n",
    "4. Wooden matches are packaged in matchboxes, and paper matches are partially cut into rows and stapled \n",
    "into matchbooks.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44a7be9",
   "metadata": {},
   "source": [
    "### Operator\n",
    "`{\"OP\": \"!\"}` -> Negation: match 0 times  \n",
    "`{\"OP\": \"?\"}` -> Negation: match 0 or 1 times  \n",
    "`{\"OP\": \"+\"}` -> Negation: match 1 or more times  \n",
    "`{\"OP\": \"*\"}` -> Negation: match 0 or more times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da75e3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string repr: 1\n",
      "matches are\n",
      "string repr: 1\n",
      "matches are\n",
      "string repr: 1\n",
      "matches are\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"TEXT\": \"match\"}, {\"TEXT\": \"is\"}]     # [\"match is\"]\n",
    "pattern = [{\"TEXT\": \"matches\"}, {\"TEXT\": \"is\"}]   # None\n",
    "\n",
    "pattern = [{\"LOWER\": \"match\"}, {\"LOWER\": \"is\"}]   # [\"match is\"]\n",
    "pattern = [{\"LOWER\": \"matches\"}, {\"LOWER\": \"is\"}] # None\n",
    "\n",
    "pattern = [{\"LEMMA\": \"match\"}, {\"LOWER\": \"are\"}] # [\"matches are\", \"matches are\", \"matches are\"]\n",
    "matcher.add(\"1\", [pattern])\n",
    "pattern = [{\"LEMMA\": \"match\"}, {\"TEXT\": \"are\"}]  # same as above\n",
    "pattern = [{\"LEMMA\": \"matches\"}, {\"LOWER\": \"are\"}] # None\n",
    "\n",
    "for match_id, start, end in matcher(doc):\n",
    "    print(f\"string repr: {nlp.vocab.strings[match_id]}\")\n",
    "    print(doc[start:end].text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b809bdc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string repr: 2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "pattern = nlp(\"ignited by frictional\")\n",
    "matcher.add(\"2\", [pattern])\n",
    "for match_id, start, end in matcher(doc):\n",
    "    print(f\"string repr: {nlp.vocab.strings[match_id]}\")\n",
    "    print(nlp.vocab.strings[match_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14f69fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 6947170135922310690\n",
      "string repr: Spacy\n"
     ]
    }
   ],
   "source": [
    "spacy_id = nlp.vocab.strings[\"Spacy\"]\n",
    "print(f\"ID: {spacy_id}\")\n",
    "print(f\"string repr: {nlp.vocab.strings[spacy_id]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3353910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8627204117787385\n",
      "sim(fast food, pizza) 0.5443140268325806\n",
      "sim((food, pizza)) 0.5924741625785828\n",
      "sim(food, dog) 0.45913576505873555\n",
      "sim(food, eat) 0.7164894513173897\n"
     ]
    }
   ],
   "source": [
    "# Similarity measure\n",
    "\n",
    "doc1 = nlp(\"I like fast food\")\n",
    "doc2 = nlp(\"I like pizza\")\n",
    "print(doc1.similarity(doc2))\n",
    "print(f\"sim({doc1[2:]}, {doc2[2]}) {doc1[2:].similarity(doc2[2])}\")\n",
    "print(f\"sim({doc1[3], doc2[2]}) {doc1[3].similarity(doc2[2])}\")\n",
    "print(f\"sim(food, dog) {nlp('food').similarity(nlp('dog'))}\")\n",
    "print(f\"sim(food, eat) {nlp('food').similarity(nlp('eat'))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5badbc5",
   "metadata": {},
   "source": [
    "## Build in pipeline components\n",
    "- `Tagger` (part of speech tagger) -> Token.tag, Token,pos\n",
    "- `Parser` (Dependency parser) -> Token.dep, Token,head, Doc.sents, Doc.noun_chunks\n",
    "- `ner` (Named entity recognizer) -> Doc.ents, Token.ent_iob, Token.ent_type\n",
    "- `textcat` (Text classifier) -> Doc.cats\n",
    "\n",
    "Pipeline defined in model's `meta.json` in order  \n",
    "- All models include a meta.json that defines the language to initialize, the pipeline component names to load as well as general meta information like the model name, version, license, data sources, author and accuracy figures (if available).  \n",
    "- To predict linguistic annotations like part-of-speech tags, dependency labels or named entities, models include binary weights.  \n",
    "- Model packages include a strings.json that stores the entries in the modelâ€™s vocabulary and the mapping to hashes. This allows spaCy to only communicate in hashes and look up the corresponding string if needed.  \n",
    "\n",
    "```json\n",
    "{\n",
    "    \"lang\": \"en\":,\n",
    "    \"name\", \"core_web_lg\",\n",
    "    \"pipeline\": [\"tagger\", \"parser\", \"ner\"]\n",
    "}\n",
    "```\n",
    "Build-in components need binary data to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6282f267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x242a59e3180>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x242a5a38770>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x242a584ce80>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x242a5705880>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x242a5aa6b40>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x242a5aa9700>)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names  # ['tok2vec', 'tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer']\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8f16e0",
   "metadata": {},
   "source": [
    "## Custom pipeline components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b21dab53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.language import Language\n",
    "\n",
    "@Language.component(\"custom_component\")\n",
    "def custom_component(doc):\n",
    "    print(f\"After tokenization, this doc has {len(doc)} tokens.\")\n",
    "    print(\"The part of speech tags are:\", [token.pos_ for token in doc])\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d12baf20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "from spacy.language import Language\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "688b8e60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['custom_component',\n",
       " 'tok2vec',\n",
       " 'tagger',\n",
       " 'parser',\n",
       " 'ner',\n",
       " 'attribute_ruler',\n",
       " 'lemmatizer']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.add_pipe(\"custom_component\", first=True)\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4adbceb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After tokenization, this doc has 10 tokens.\n",
      "The part of speech tags are: ['', '', '', '', '', '', '', '', '', '']\n",
      "1. Let's learn Spacy for $0 billion\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"1. Let's learn Spacy for $0 billion\")\n",
    "print(doc.text)\n",
    "# for token in doc:\n",
    "#     print(token.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5aad0c9",
   "metadata": {},
   "source": [
    "## Custom Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35785827",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc, Token, Span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5703f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attribute Extensions\n",
    "Doc.set_extension(\"title\", default=None)\n",
    "Token.set_extension(\"is_important\", default=False)\n",
    "Span.set_extension(\"weight\", default=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c2b5156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After tokenization, this doc has 10 tokens.\n",
      "The part of speech tags are: ['', '', '', '', '', '', '', '', '', '']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Statement', True, 0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"1. Let's learn Spacy for $0 billion\")\n",
    "doc._.title = \"Statement\"\n",
    "doc[5]._.is_important = True\n",
    "doc[2:6]._.weight = 0.5\n",
    "doc._.title, doc[5]._.is_important, doc[2:4]._.weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "239486ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After tokenization, this doc has 10 tokens.\n",
      "The part of speech tags are: ['', '', '', '', '', '', '', '', '', '']\n",
      "\"1\" (is imp): False\n",
      "\".\" (is imp): False\n",
      "\"Let\" (is imp): True\n",
      "\"'s\" (is imp): False\n",
      "\"learn\" (is imp): False\n",
      "\"Spacy\" (is imp): True\n",
      "\"for\" (is imp): False\n",
      "\"$\" (is imp): True\n",
      "\"0\" (is imp): False\n",
      "\"billion\" (is imp): False\n"
     ]
    }
   ],
   "source": [
    "Token.remove_extension(\"is_important\")\n",
    "Token.set_extension(\"is_important\", getter=lambda token : token.is_title or token.is_currency)\n",
    "\n",
    "doc = nlp(\"1. Let's learn Spacy for $0 billion\")\n",
    "for token in doc:\n",
    "    print(f\"\\\"{token}\\\" (is imp): {token._.is_important}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d16bdf51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After tokenization, this doc has 10 tokens.\n",
      "The part of speech tags are: ['', '', '', '', '', '', '', '', '', '']\n",
      "\"1.\" (has title): False\n",
      "\"1. Let\" (has title): True\n",
      "\"for $0 billion\" (has currency): True\n",
      "\"1. Let's learn Spacy for $0 billion\" (has important): True\n"
     ]
    }
   ],
   "source": [
    "Span.set_extension(\"has_title\", getter=lambda span : any(token.is_title for token in span))\n",
    "Span.set_extension(\"has_currency\", getter=lambda span : any(token.is_currency for token in span))\n",
    "Span.set_extension(\"has_important\", getter=lambda span : any((token.is_title or token.is_currency) for token in span))\n",
    "\n",
    "doc = nlp(\"1. Let's learn Spacy for $0 billion\")\n",
    "print(f\"\\\"{doc[:2]}\\\" (has title): {doc[:2]._.has_title}\")\n",
    "print(f\"\\\"{doc[:3]}\\\" (has title): {doc[:3]._.has_title}\")\n",
    "print(f\"\\\"{doc[6:]}\\\" (has currency): {doc[6:]._.has_currency}\")\n",
    "print(f\"\\\"{doc[:]}\\\" (has important): {doc[:]._.has_currency}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b78d007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method Extensions\n",
    "Doc.set_extension(\"has_token\", method=lambda doc, token_txt: token_txt in [token.text for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36b6e5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After tokenization, this doc has 10 tokens.\n",
      "The part of speech tags are: ['', '', '', '', '', '', '', '', '', '']\n",
      "Is \"billion\" in doc: True\n",
      "Is \"billions\" in doc: False\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"1. Let's learn Spacy for $0 billion\")\n",
    "print(f\"Is \\\"billion\\\" in doc: {doc._.has_token('billion')}\")\n",
    "print(f\"Is \\\"billions\\\" in doc: {doc._.has_token('billions')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4312cbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44be5333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing large volume of Text\n",
    "\n",
    "# BAD:\n",
    "# docs = [nlp(text) for text in LOTS_OF_TEXTS]\n",
    "\n",
    "# GOOD:\n",
    "# docs = list(nlp.pipe(LOTS_OF_TEXTS))\n",
    "#  It process the text as a String and \"yields\" the Docs Objects, it batches up the text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b5ba0520",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc\n",
    "Doc.set_extension(\"id\", default=None)\n",
    "Doc.set_extension(\"page_number\", default=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad94ea46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After tokenization, this doc has 11 tokens.\n",
      "The part of speech tags are: ['', '', '', '', '', '', '', '', '', '', '']\n",
      "After tokenization, this doc has 10 tokens.\n",
      "The part of speech tags are: ['', '', '', '', '', '', '', '', '', '']\n",
      "1. Let's Learn Spacy in $0 billion! 1\n",
      "2. Go and get some BTC worth $100K. 2\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (\"1. Let's Learn Spacy in $0 billion!\", dict(id=1, page_number=1)),\n",
    "    (\"2. Go and get some BTC worth $100K.\", dict(id=1, page_number=2)),\n",
    "]\n",
    "\n",
    "for doc, context in nlp.pipe(data, as_tuples=True):\n",
    "    print(doc.text, context[\"page_number\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1faa4553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After tokenization, this doc has 11 tokens.\n",
      "The part of speech tags are: ['', '', '', '', '', '', '', '', '', '', '']\n",
      "After tokenization, this doc has 10 tokens.\n",
      "The part of speech tags are: ['', '', '', '', '', '', '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "for doc, context in nlp.pipe(data, as_tuples=True):\n",
    "    doc._.id=context[\"id\"]\n",
    "    doc._.page_number=context[\"page_number\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6343781f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After tokenization, this doc has 3 tokens.\n",
      "The part of speech tags are: ['', '', '']\n"
     ]
    }
   ],
   "source": [
    "# Using only tokenizer\n",
    "doc = nlp(\"Hello World!\")  # BAD: it will run whole pipeline\n",
    "doc = nlp.make_doc(\"Hello World!\")  # GOOD: it only tokenize the sentence don't run the whole pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7cab795a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After tokenization, this doc has 11 tokens.\n",
      "The part of speech tags are: ['', '', '', '', '', '', '', '', '', '', '']\n",
      "1. Let's Learn Spacy in $0 billion!\n",
      "['custom_component', 'tok2vec', 'ner', 'attribute_ruler']\n",
      "After tokenization, this doc has 11 tokens.\n",
      "The part of speech tags are: ['', '', '', '', '', '', '', '', '', '', '']\n",
      "1. Let's Learn Spacy in $0 billion!\n",
      "['custom_component', 'tok2vec', 'tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer']\n"
     ]
    }
   ],
   "source": [
    "# Disabling the pipeline components\n",
    "with nlp.disable_pipes(\"tagger\", \"parser\", \"lemmatizer\"):\n",
    "    doc = nlp(\"1. Let's Learn Spacy in $0 billion!\")\n",
    "    print(doc.text)\n",
    "    print(nlp.pipe_names)\n",
    "doc = nlp(\"1. Let's Learn Spacy in $0 billion!\")\n",
    "print(doc.text)\n",
    "print(nlp.pipe_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
